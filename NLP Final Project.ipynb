{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6e2c85",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dbce6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "version = sys.version # Python version\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fd50daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n",
      "8500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) # PyTorch version\n",
    "print(torch.backends.cudnn.version()) # cuDNN version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e587df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_19:00:59_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version # CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6af8cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "count = torch.cuda.device_count()\n",
    "for i in range(count): # Available GPUs\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    print(i, \":\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e3144",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f5426608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0f6a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7b08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27564012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import miditoolkit\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "import hashlib\n",
    "from multiprocessing import Pool, Lock, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3389be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\Jupyter\\content\\muzic\\musicbert\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = r\"C:/Users/chowd/Jupyter/content/\"\n",
    "os.chdir(path)\n",
    "os.chdir(\"./muzic/musicbert/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6141df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable_cp = False\n",
      "mask_strategy = ['bar']\n",
      "convert_encoding = OCTMIDI\n",
      "crop_length = None\n"
     ]
    }
   ],
   "source": [
    "from musicbert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2fd6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playMidi(filename):\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(filename)\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    s = midi.translate.midiFileToStream(mf)\n",
    "    s.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4c60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_midi_file(sample_midi_path: str):\n",
    "    midi_obj = miditoolkit.midi.parser.MidiFile(sample_midi_path)\n",
    "    midi_name = sample_midi_path.split('/')[-1].split('.')[0]\n",
    "    return midi_obj, midi_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8f699",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748b398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_max = 256 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe2aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_resolution = 16  # per beat (quarter note)\n",
    "\n",
    "velocity_quant = 4\n",
    "tempo_quant = 12  # 2 ** (1 / 12)\n",
    "min_tempo = 16\n",
    "max_tempo = 256\n",
    "duration_max = 8  # 2 ** 8 * beat\n",
    "max_ts_denominator = 6  # x/1 x/2 x/4 ... x/64\n",
    "max_notes_per_bar = 2  # 1/64 ... 128/64\n",
    "beat_note_factor = 4  # In MIDI format a note is always 4 beats\n",
    "deduplicate = True\n",
    "filter_symbolic = False\n",
    "filter_symbolic_ppl = 16\n",
    "trunc_pos = 2 ** 16  # approx 30 minutes (1024 measures)\n",
    "sample_len_max = 1000  # window length max\n",
    "sample_overlap_rate = 4\n",
    "ts_filter = False\n",
    "pool_num = 24\n",
    "max_inst = 127\n",
    "max_pitch = 127\n",
    "max_velocity = 127\n",
    "\n",
    "data_zip = None\n",
    "output_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace29bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dict = dict()\n",
    "ts_list = list()\n",
    "for i in range(0, max_ts_denominator + 1):  # 1 ~ 64\n",
    "    for j in range(1, ((2 ** i) * max_notes_per_bar) + 1):\n",
    "        ts_dict[(j, 2 ** i)] = len(ts_dict)\n",
    "        ts_list.append((j, 2 ** i))\n",
    "dur_enc = list()\n",
    "dur_dec = list()\n",
    "for i in range(duration_max):\n",
    "    for j in range(pos_resolution):\n",
    "        dur_dec.append(len(dur_enc))\n",
    "        for k in range(2 ** i):\n",
    "            dur_enc.append(len(dur_dec) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b587e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2e(x):\n",
    "    assert x in ts_dict, 'unsupported time signature: ' + str(x)\n",
    "    return ts_dict[x]\n",
    "\n",
    "\n",
    "def e2t(x):\n",
    "    return ts_list[x]\n",
    "\n",
    "\n",
    "def d2e(x):\n",
    "    return dur_enc[x] if x < len(dur_enc) else dur_enc[-1]\n",
    "\n",
    "\n",
    "def e2d(x):\n",
    "    return dur_dec[x] if x < len(dur_dec) else dur_dec[-1]\n",
    "\n",
    "\n",
    "def v2e(x):\n",
    "    return x // velocity_quant\n",
    "\n",
    "\n",
    "def e2v(x):\n",
    "    return (x * velocity_quant) + (velocity_quant // 2)\n",
    "\n",
    "\n",
    "def b2e(x):\n",
    "    x = max(x, min_tempo)\n",
    "    x = min(x, max_tempo)\n",
    "    x = x / min_tempo\n",
    "    e = round(math.log2(x) * tempo_quant)\n",
    "    return e\n",
    "\n",
    "\n",
    "def e2b(x):\n",
    "    return 2 ** (x / tempo_quant) * min_tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6418566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_signature_reduce(numerator, denominator):\n",
    "    # reduction (when denominator is too large)\n",
    "    while denominator > 2 ** max_ts_denominator and denominator % 2 == 0 and numerator % 2 == 0:\n",
    "        denominator //= 2\n",
    "        numerator //= 2\n",
    "    # decomposition (when length of a bar exceed max_notes_per_bar)\n",
    "    while numerator > max_notes_per_bar * denominator:\n",
    "        for i in range(2, numerator + 1):\n",
    "            if numerator % i == 0:\n",
    "                numerator //= i\n",
    "                break\n",
    "    return numerator, denominator\n",
    "\n",
    "\n",
    "def writer(output_str_list, output_file):\n",
    "    # note: parameter \"file_name\" is reserved for patching\n",
    "    with open(output_file, 'a') as f:\n",
    "        for output_str in output_str_list:\n",
    "            f.write(output_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac1e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(encoding):\n",
    "    # add i[4] and i[5] for stricter match\n",
    "    midi_tuple = tuple((i[2], i[3]) for i in encoding)\n",
    "    midi_hash = hashlib.md5(str(midi_tuple).encode('ascii')).hexdigest()\n",
    "    return midi_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da17b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIDI_to_encoding(midi_obj):\n",
    "    def time_to_pos(t):\n",
    "        return round(t * pos_resolution / midi_obj.ticks_per_beat)\n",
    "    notes_start_pos = [time_to_pos(j.start)\n",
    "                       for i in midi_obj.instruments for j in i.notes]\n",
    "    if len(notes_start_pos) == 0:\n",
    "        return list()\n",
    "    max_pos = min(max(notes_start_pos) + 1, trunc_pos)\n",
    "    pos_to_info = [[None for _ in range(4)] for _ in range(\n",
    "        max_pos)]  # (Measure, TimeSig, Pos, Tempo)\n",
    "    tsc = midi_obj.time_signature_changes\n",
    "    tpc = midi_obj.tempo_changes\n",
    "    for i in range(len(tsc)):\n",
    "        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):\n",
    "            if j < len(pos_to_info):\n",
    "                pos_to_info[j][1] = t2e(time_signature_reduce(\n",
    "                    tsc[i].numerator, tsc[i].denominator))\n",
    "    for i in range(len(tpc)):\n",
    "        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):\n",
    "            if j < len(pos_to_info):\n",
    "                pos_to_info[j][3] = b2e(tpc[i].tempo)\n",
    "    for j in range(len(pos_to_info)):\n",
    "        if pos_to_info[j][1] is None:\n",
    "            # MIDI default time signature\n",
    "            pos_to_info[j][1] = t2e(time_signature_reduce(4, 4))\n",
    "        if pos_to_info[j][3] is None:\n",
    "            pos_to_info[j][3] = b2e(120.0)  # MIDI default tempo (BPM)\n",
    "    cnt = 0\n",
    "    bar = 0\n",
    "    measure_length = None\n",
    "    for j in range(len(pos_to_info)):\n",
    "        ts = e2t(pos_to_info[j][1])\n",
    "        if cnt == 0:\n",
    "            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]\n",
    "        pos_to_info[j][0] = bar\n",
    "        pos_to_info[j][2] = cnt\n",
    "        cnt += 1\n",
    "        if cnt >= measure_length:\n",
    "            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(\n",
    "                j)\n",
    "            cnt -= measure_length\n",
    "            bar += 1\n",
    "    encoding = []\n",
    "    start_distribution = [0] * pos_resolution\n",
    "    for inst in midi_obj.instruments:\n",
    "        for note in inst.notes:\n",
    "            if time_to_pos(note.start) >= trunc_pos:\n",
    "                continue\n",
    "            start_distribution[time_to_pos(note.start) % pos_resolution] += 1\n",
    "            info = pos_to_info[time_to_pos(note.start)]\n",
    "            encoding.append((info[0], info[2], max_inst + 1 if inst.is_drum else inst.program, note.pitch + max_pitch +\n",
    "                             1 if inst.is_drum else note.pitch, d2e(time_to_pos(note.end) - time_to_pos(note.start)), v2e(note.velocity), info[1], info[3]))\n",
    "    if len(encoding) == 0:\n",
    "        return list()\n",
    "    tot = sum(start_distribution)\n",
    "    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) *\n",
    "                          math.log2((x / tot)) for x in start_distribution))\n",
    "    # filter unaligned music\n",
    "    if filter_symbolic:\n",
    "        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(\n",
    "            start_ppl)\n",
    "    encoding.sort()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a752aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_encoding(s):\n",
    "    encoding = [int(i[3: -1]) for i in s.split() if 's' not in i]\n",
    "    tokens_per_note = 8\n",
    "    assert len(encoding) % tokens_per_note == 0\n",
    "    encoding = [tuple(encoding[i + j] for j in range(tokens_per_note))\n",
    "                for i in range(0, len(encoding), tokens_per_note)]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ad4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_to_str(e, bar_max = bar_max):\n",
    "    bar_index_offset = 0\n",
    "    p = 0\n",
    "    tokens_per_note = 8\n",
    "    return ' '.join((['<s>'] * tokens_per_note)\n",
    "                    + ['<{}-{}>'.format(j, k if j > 0 else k + bar_index_offset) for i in e[p: p +\n",
    "                                                                                            sample_len_max] if i[0] + bar_index_offset < bar_max for j, k in enumerate(i)]\n",
    "                    + (['</s>'] * (tokens_per_note\n",
    "                                   - 1)))   # 8 - 1 for append_eos functionality of binarizer in fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c74e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_file = Lock()\n",
    "lock_write = Lock()\n",
    "lock_set = Lock()\n",
    "manager = Manager()\n",
    "midi_dict = manager.dict()\n",
    "\n",
    "e = None\n",
    "midi_file = None\n",
    "file_name = None\n",
    "\n",
    "def encode_midi(midi_file):\n",
    "    midi_obj = miditoolkit.midi.parser.MidiFile(file=midi_file)\n",
    "    \n",
    "    assert all(0 <= j.start < 2 ** 31 and 0 <= j.end < 2 **\n",
    "               31 for i in midi_obj.instruments for j in i.notes), 'bad note time'\n",
    "    assert all(0 < j.numerator < 2 ** 31 and 0 < j.denominator < 2 **\n",
    "               31 for j in midi_obj.time_signature_changes), 'bad time signature value'\n",
    "    assert 0 < midi_obj.ticks_per_beat < 2 ** 31, 'bad ticks per beat'\n",
    "    \n",
    "    midi_notes_count = sum(len(inst.notes) for inst in midi_obj.instruments)\n",
    "    if midi_notes_count == 0:\n",
    "        print('ERROR(BLANK): ' + file_name + '\\n', end='')\n",
    "        return None\n",
    "    \n",
    "    e = MIDI_to_encoding(midi_obj)\n",
    "    length = len(e)\n",
    "    \n",
    "    if length == 0:\n",
    "        print('ERROR(BLANK): ' + file_name + '\\n', end='')\n",
    "        return None\n",
    "    if ts_filter:\n",
    "        allowed_ts = t2e(time_signature_reduce(4, 4))\n",
    "        if not all(i[6] == allowed_ts for i in e):\n",
    "            print('ERROR(TSFILT): ' + file_name + '\\n', end='')\n",
    "            return None\n",
    "\n",
    "    dup_file_name = ''\n",
    "    midi_hash = '0' * 32\n",
    "\n",
    "    midi_hash = get_hash(e)\n",
    "    lock_set.acquire()\n",
    "    if midi_hash in midi_dict:\n",
    "        dup_file_name = midi_dict[midi_hash]\n",
    "        duplicated = True\n",
    "    else:\n",
    "        midi_dict[midi_hash] = file_name\n",
    "        duplicated = False\n",
    "    lock_set.release()\n",
    "\n",
    "    output_str_list = []\n",
    "    sample_step = max(round(sample_len_max / sample_overlap_rate), 1)\n",
    "    for p in range(0 - random.randint(0, sample_len_max - 1), length, sample_step):\n",
    "        L = max(p, 0)\n",
    "        R = min(p + sample_len_max, length) - 1\n",
    "        bar_index_list = [e[i][0] for i in range(L, R + 1) if e[i][0] is not None]\n",
    "        bar_index_min = 0\n",
    "        bar_index_max = 0\n",
    "        if len(bar_index_list) > 0:\n",
    "            bar_index_min = min(bar_index_list)\n",
    "            bar_index_max = max(bar_index_list)\n",
    "        offset_lower_bound = -bar_index_min\n",
    "        offset_upper_bound = bar_max - 1 - bar_index_max\n",
    "        # to make bar index distribute in [0, bar_max)\n",
    "        bar_index_offset = random.randint(\n",
    "            offset_lower_bound, offset_upper_bound) if offset_lower_bound <= offset_upper_bound else offset_lower_bound\n",
    "        e_segment = []\n",
    "        for i in e[L: R + 1]:\n",
    "            if i[0] is None or i[0] + bar_index_offset < bar_max:\n",
    "                e_segment.append(i)\n",
    "            else:\n",
    "                break\n",
    "        tokens_per_note = 8\n",
    "        output_words = (['<s>'] * tokens_per_note) \\\n",
    "            + [('<{}-{}>'.format(j, k if j > 0 else k + bar_index_offset) \n",
    "                if k is not None else '<unk>') for i in e_segment for j, k in enumerate(i)] \\\n",
    "            + (['</s>'] * (tokens_per_note - 1)\n",
    "               )  # tokens_per_note - 1 for append_eos functionality of binarizer in fairseq\n",
    "        output_str_list.append(' '.join(output_words))\n",
    "\n",
    "    # no empty\n",
    "    if not all(len(i.split()) > tokens_per_note * 2 - 1 for i in output_str_list):\n",
    "        print('ERROR(ENCODE): ' + file_name + ' ' + str(e) + '\\n', end='')\n",
    "        return None\n",
    "\n",
    "    return output_str_list\n",
    "    \n",
    "\n",
    "\n",
    "def F(data_path, output_path):\n",
    "    midi_file_list = dict()\n",
    "    \n",
    "    lock_file.acquire()\n",
    "    \n",
    "    data_zip = zipfile.ZipFile(data_path, 'r')\n",
    "    file_name_list = data_zip.namelist()\n",
    "    \n",
    "    lock_file.release()\n",
    "    \n",
    "    lock_write.acquire()\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        with data_zip.open(file_name) as f:\n",
    "            if file_name[-3:] != \"mid\":\n",
    "                continue\n",
    "            # this may fail due to unknown bug\n",
    "            midi_file = io.BytesIO(f.read())\n",
    "        \n",
    "        encoded_midi = encode_midi(midi_file)\n",
    "        output_file = output_path + \"/\" + file_name.split(\"/\")[-1].split(\".\")[0] + \".txt\"\n",
    "        writer(encoded_midi, output_file)\n",
    "    \n",
    "    lock_write.release()\n",
    "   \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a317f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "data_path = r'./segmented_midi.zip'\n",
    "output_path = r'./encoded_tokens'\n",
    "result = F(data_path, output_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1ee9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_LIST = [\"Stable beat\", \"Mechanical Tempo\", \"Intensional\", \"Regular beat change\", \"Long\", \"Cushioned\", \"Saturated (wet)\", \"Clean\", \"Subtle change\", \"Even\", \"Rich\", \"Bright\", \n",
    "\"Pure\", \"Soft\", \"Sophisticated(mellow)\", \"balanced\", \"Large range of dynamic\", \"Fast paced\", \"Flowing\", \"Swing(Flexible)\", \"Flat\", \"Harmonious\", \"Optimistic(pleasant)\", \"HIgh Energy\", \n",
    "\"Dominant(forceful)\", \"Imaginative\", \"Ethereal\", \"Convincing\"]\n",
    "LABEL_MAP = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "PIANIST_MAP = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dca12c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_maxima(data):\n",
    "    \"\"\"Calculate maxima using kernel density estimation\"\"\"\n",
    "    if len(set(data))<=1: # all datas are equal\n",
    "        return data[0]\n",
    "    kde = gaussian_kde(data)\n",
    "    no_samples = 50\n",
    "    samples = np.linspace(min(data), max(data), no_samples)\n",
    "    probs = kde.evaluate(samples)\n",
    "    #maxima_index = probs.argmax()\n",
    "    # in case if more than 1 argmaxs\n",
    "    winner = np.argwhere(probs == np.amax(probs))\n",
    "    maxima = np.average(samples[winner.flatten()])\n",
    "    return maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a9183df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 846/846 [00:02<00:00, 416.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', 0), ('2', 1), ('4', 2), ('5', 3), ('Score', 4), ('0', 5), ('3', 6), ('9', 7), ('10', 8), ('7', 9), ('8', 10), ('6', 11), ('12', 12)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<00:00, 846869.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def midi_label_map_apex(file, target):\n",
    "\n",
    "    csvreader = csv.reader(file)\n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # sort by each segments\n",
    "    music_label_map = defaultdict(list)\n",
    "    for row in rows:\n",
    "        user = row[0]\n",
    "        file_name = row[2].split(\".\")[0]\n",
    "        #label_row = row[3:-2]\n",
    "        label_row = [row[3]] + row[7:-2] # skip 1-2 ~ 1-3\n",
    "        for idx, elem in enumerate(label_row):\n",
    "            if elem == \"\":\n",
    "                label_row[idx] = 0.0\n",
    "            else:\n",
    "                label_row[idx] = float(elem)\n",
    "        # skip 0\n",
    "        if 0.0 in label_row:\n",
    "            continue\n",
    "        else:\n",
    "            music_label_map[file_name].append(label_row)\n",
    "\n",
    "    music_label_map_apex = dict()\n",
    "\n",
    "    # kernel density estimation\n",
    "    for key, annot_list in tqdm(music_label_map.items()):\n",
    "        annot_list = np.array(annot_list).transpose()\n",
    "        maxima = np.array([estimate_maxima(row)/7 for row in annot_list])\n",
    "        maxima = maxima.transpose().tolist()\n",
    "        music_label_map_apex[key] = maxima\n",
    "\n",
    "    # add pianist info\n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        if key.split(\"_\")[-2] not in PIANIST_MAP:\n",
    "            PIANIST_MAP[key.split(\"_\")[-2]] = len(PIANIST_MAP)\n",
    "    print(PIANIST_MAP)\n",
    "    \n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        music_label_map_apex[key].append(PIANIST_MAP[key.split(\"_\")[-2]])\n",
    "\n",
    "    json.dump(music_label_map_apex, target)\n",
    "\n",
    "os.chdir(path)\n",
    "file = open('./total.csv', encoding=\"utf-8\")\n",
    "target = open(\"./muzic/musicbert/midi_label_map_apex_reg_cls.json\", 'w')\n",
    "midi_label_map_apex(file, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "773b6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fba1ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<00:00, 16918.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('1', 0), ('2', 1), ('4', 2), ('5', 3), ('Score', 4), ('0', 5), ('3', 6), ('9', 7), ('10', 8), ('7', 9), ('8', 10), ('6', 11), ('12', 12)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 846/846 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def midi_label_map_apex_mode(file, target):\n",
    "\n",
    "    csvreader = csv.reader(file)\n",
    "    header = []\n",
    "    header = next(csvreader)\n",
    "\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "\n",
    "    # sort by each segments\n",
    "    music_label_map = defaultdict(list)\n",
    "    for row in rows:\n",
    "        user = row[0]\n",
    "        file_name = row[2].split(\".\")[0]\n",
    "        #label_row = row[3:-2]\n",
    "        label_row = [row[3]] + row[7:-2] # skip 1-2 ~ 1-3\n",
    "        for idx, elem in enumerate(label_row):\n",
    "            if elem == \"\":\n",
    "                label_row[idx] = 0.0\n",
    "            else:\n",
    "                label_row[idx] = float(elem)\n",
    "        # skip 0\n",
    "        if 0.0 in label_row:\n",
    "            continue\n",
    "        else:\n",
    "            music_label_map[file_name].append(label_row)\n",
    "\n",
    "    music_label_map_apex = dict()\n",
    "\n",
    "    # kernel density estimation\n",
    "    for key, annot_list in tqdm(music_label_map.items()):\n",
    "        annot_list = np.array(annot_list).transpose()\n",
    "        maxima = np.array([statistics.mode(row) for row in annot_list])\n",
    "        maxima = maxima.transpose().tolist()\n",
    "        music_label_map_apex[key] = maxima\n",
    "\n",
    "    # add pianist info\n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        if key.split(\"_\")[-2] not in PIANIST_MAP:\n",
    "            PIANIST_MAP[key.split(\"_\")[-2]] = len(PIANIST_MAP)\n",
    "    print(PIANIST_MAP)\n",
    "    \n",
    "    for key, annot_list in tqdm(music_label_map_apex.items()):\n",
    "        music_label_map_apex[key].append(PIANIST_MAP[key.split(\"_\")[-2]])\n",
    "\n",
    "    json.dump(music_label_map_apex, target)\n",
    "\n",
    "os.chdir(path)\n",
    "file = open('./total.csv', encoding=\"utf-8\")\n",
    "target = open(\"./muzic/musicbert/midi_label_map_apex_reg_cls_mode.json\", 'w')\n",
    "midi_label_map_apex_mode(file, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72b52cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117281a",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "f2b2aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaHubInterface(\n",
      "  (model): MusicBERTModel(\n",
      "    (encoder): MusicBERTEncoder(\n",
      "      (sentence_encoder): OctupleEncoder(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (embed_tokens): Embedding(1237, 768, padding_idx=1)\n",
      "        (embed_positions): LearnedPositionalEmbedding(8194, 768, padding_idx=1)\n",
      "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): TransformerSentenceEncoderLayer(\n",
      "            (dropout_module): FairseqDropout()\n",
      "            (activation_dropout_module): FairseqDropout()\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (dropout_module): FairseqDropout()\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (downsampling): Sequential(\n",
      "          (0): Linear(in_features=6144, out_features=768, bias=True)\n",
      "        )\n",
      "        (upsampling): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=6144, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (lm_head): RobertaLMHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (classification_heads): ModuleDict(\n",
      "      (topmagd_head): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=13, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "os.chdir('./muzic/musicbert/')\n",
    "roberta_base = MusicBERTModel.from_pretrained('.', \n",
    "  checkpoint_file = './checkpoints/checkpoint_last_musicbert_base_w_genre_head.pt',\n",
    "#  user_dir='C:/Users/chowd/Jupyter/content/muzic/musicbert/musicbert'    # activate the MusicBERT plugin with this keyword\n",
    ")\n",
    "#,\n",
    " # data_name_or_path = '.')\n",
    "print(roberta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "aca6e574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base.model.max_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "835de5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OctupleEncoder(\n",
       "  (dropout_module): FairseqDropout()\n",
       "  (embed_tokens): Embedding(1237, 768, padding_idx=1)\n",
       "  (embed_positions): LearnedPositionalEmbedding(8194, 768, padding_idx=1)\n",
       "  (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerSentenceEncoderLayer(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (activation_dropout_module): FairseqDropout()\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (downsampling): Sequential(\n",
       "    (0): Linear(in_features=6144, out_features=768, bias=True)\n",
       "  )\n",
       "  (upsampling): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=6144, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base1 = roberta_base.model.encoder.sentence_encoder\n",
    "roberta_base.cuda()\n",
    "roberta_base.eval()\n",
    "roberta_base1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3347f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, param) in enumerate(roberta_base1.named_parameters()):\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b5fe0",
   "metadata": {},
   "source": [
    "## Generate Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0da70b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018\n"
     ]
    }
   ],
   "source": [
    "### Get midi file list \n",
    "data_path = r'C:/Users/chowd/Jupyter/content/'\n",
    "file_name = r'segmented_midi.zip'\n",
    "\n",
    "lock_file.acquire()\n",
    "data_zip = zipfile.ZipFile(data_path + file_name, 'r')\n",
    "file_name_list = data_zip.namelist()\n",
    "lock_file.release()\n",
    "\n",
    "file_name_list = [data_path + x for x in file_name_list if x[-3:] == \"mid\"]\n",
    "print(len(file_name_list))\n",
    "#print(file_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2223e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get midi data\n",
    "midi_data = dict()\n",
    "for midi_path in file_name_list:\n",
    "    name = midi_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    midi_obj = miditoolkit.midi.parser.MidiFile(midi_path)\n",
    "    tokenized_string = encoding_to_str(MIDI_to_encoding(midi_obj))\n",
    "    encoded_tensor = roberta_base.task.label_dictionary.encode_line(tokenized_string)\n",
    "    tensor = torch.Tensor(encoded_tensor)\n",
    "    reshaped_tensor = torch.reshape(tensor, (-1, 8)).cuda()\n",
    "    midi_data[name] = reshaped_tensor\n",
    "len(midi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fa5e039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get labels\n",
    "os.chdir(path)\n",
    "os.chdir('./muzic/musicbert/')\n",
    "rows = list()\n",
    "data = None\n",
    "with open(\"midi_label_map_apex_reg_cls.json\", 'r') as file:\n",
    "    label_data = json.load(file)\n",
    "#data1 = json.dumps(data)\n",
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83c9cd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list()\n",
    "data = None\n",
    "with open(\"midi_label_map_apex_reg_cls_mode.json\", 'r') as file:\n",
    "    label_data_mode = json.load(file)\n",
    "#data1 = json.dumps(data)\n",
    "len(label_data_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e4cf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799\n"
     ]
    }
   ],
   "source": [
    "### Sort out data\n",
    "midi_keys = set(midi_data.keys())\n",
    "label_keys = set(label_data.keys())\n",
    "file_list = midi_keys.intersection(label_keys)\n",
    "length = len(file_list)\n",
    "x = list()\n",
    "y = list()\n",
    "y_mode = list()\n",
    "for file_name in file_list:\n",
    "    x.append(torch.Tensor(midi_data[file_name]).cuda())\n",
    "    y.append(torch.Tensor(label_data[file_name]).cuda())\n",
    "    y_mode.append(torch.Tensor(label_data_mode[file_name]).cuda())\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9281c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define dataset class\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, midi_data, label_data, length):\n",
    "        self.x = midi_data\n",
    "        self.y = label_data\n",
    "        self.len = length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75f8daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataSet(x, y, length)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2b2e1",
   "metadata": {},
   "source": [
    "## Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7509cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(func, x):\n",
    "    y = x[0][0]\n",
    "    y = torch.permute(y, (1,0,2))\n",
    "    y = torch.reshape(y, (-1, 8*768))\n",
    "    y = torch.stack([\n",
    "        func(x) for x in torch.unbind(y, dim=0)\n",
    "    ], dim=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f21ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model,\n",
    "        input_dim,\n",
    "        num_classes,\n",
    "        activation_fn,\n",
    "        pooler_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
    "        self.dense = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pretrained_model.forward(x, last_state_only=True)\n",
    "        y = reshape(self.pretrained_model.downsampling, y)\n",
    "        y = y[0]  # take <s> token (equiv. to [CLS])\n",
    "        y = self.dropout(y)\n",
    "        y = self.dense(y)\n",
    "        return y\n",
    "    \n",
    "    def freeze_pretrained_model(self):\n",
    "        for i, (name, param) in enumerate(self.pretrained_model.named_parameters()):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_pretrained_model(self):\n",
    "        for i, (name, param) in enumerate(self.pretrained_model.named_parameters()):\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a55218c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "\n",
    "dataset = MyDataSet(x, y_mode, length)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(dataset=dataset,\n",
    "                                               lengths=[train_size, val_size, test_size],\n",
    "                                               generator=generator)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5fa9c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (pretrained_model): OctupleEncoder(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(1237, 768, padding_idx=1)\n",
       "    (embed_positions): LearnedPositionalEmbedding(8194, 768, padding_idx=1)\n",
       "    (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (downsampling): Sequential(\n",
       "      (0): Linear(in_features=6144, out_features=768, bias=True)\n",
       "    )\n",
       "    (upsampling): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=6144, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dense): Linear(in_features=768, out_features=175, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(pretrained_model = roberta_base1,\n",
    "                  input_dim = 768,\n",
    "                  num_classes = 7*25,\n",
    "                  activation_fn = nn.ReLU(),\n",
    "                  pooler_dropout = 0.1\n",
    "                  )\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c06e6c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train accuracy: 0.27524, Validation accuracy: 0.28911\n",
      "Epoch: 1, Train accuracy: 0.28263, Validation accuracy: 0.27646\n",
      "Epoch: 2, Train accuracy: 0.27674, Validation accuracy: 0.27443\n",
      "Epoch: 3, Train accuracy: 0.28213, Validation accuracy: 0.28051\n",
      "Epoch: 4, Train accuracy: 0.28401, Validation accuracy: 0.28506\n",
      "Epoch: 5, Train accuracy: 0.28457, Validation accuracy: 0.28911\n",
      "Epoch: 6, Train accuracy: 0.28013, Validation accuracy: 0.27797\n",
      "Epoch: 7, Train accuracy: 0.28282, Validation accuracy: 0.26582\n",
      "Epoch: 8, Train accuracy: 0.27537, Validation accuracy: 0.28759\n",
      "Epoch: 9, Train accuracy: 0.27994, Validation accuracy: 0.26430\n",
      "Epoch: 10, Train accuracy: 0.28407, Validation accuracy: 0.27696\n",
      "Epoch: 11, Train accuracy: 0.29252, Validation accuracy: 0.27848\n",
      "Epoch: 12, Train accuracy: 0.28670, Validation accuracy: 0.27595\n",
      "Epoch: 13, Train accuracy: 0.28363, Validation accuracy: 0.28304\n",
      "Epoch: 14, Train accuracy: 0.28050, Validation accuracy: 0.28051\n",
      "Epoch: 15, Train accuracy: 0.27950, Validation accuracy: 0.28405\n",
      "Epoch: 16, Train accuracy: 0.28557, Validation accuracy: 0.28709\n",
      "Epoch: 17, Train accuracy: 0.27937, Validation accuracy: 0.27696\n",
      "Epoch: 18, Train accuracy: 0.28626, Validation accuracy: 0.27544\n",
      "Epoch: 19, Train accuracy: 0.28357, Validation accuracy: 0.29367\n",
      "Test accuracy: 0.28049\n"
     ]
    }
   ],
   "source": [
    "autograd.set_detect_anomaly(True)\n",
    "epoch = 20\n",
    "batch_size = 8\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "test_loss = 0\n",
    "for epoch in range(epoch):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    for (i, data) in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "        y_pred = torch.reshape(y_pred, (-1,7))\n",
    "        y_pred_mode = torch.argmax(y_pred, 1)\n",
    "        label = labels[0][:-1]-1\n",
    "        correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss_sum = loss_sum + loss\n",
    "        if (len(train_loader) - i) % batch_size == 1:\n",
    "            loss_sum = loss_sum / len(train_loader)\n",
    "            train_loss = loss_sum.item()\n",
    "            train_acc = correct / (len(train_loader) * 25)\n",
    "            optimizer.zero_grad()\n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for (i, data) in enumerate(val_loader):\n",
    "            count = count + 1\n",
    "            inputs, labels = data\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            y_pred = torch.reshape(y_pred, (-1,7))\n",
    "            y_pred_mode = torch.argmax(y_pred, 1)\n",
    "            label = labels[0][:-1]-1\n",
    "            correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        val_acc = correct / (len(val_loader) * 25)\n",
    "    print(\"Epoch: {0}, Train accuracy: {1:0.5f}, Validation accuracy: {2:0.5f}\".format(epoch, train_acc, val_acc))\n",
    "with torch.no_grad():\n",
    "        correct = 0\n",
    "        for (i, data) in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            y_pred = torch.reshape(y_pred, (-1,7))\n",
    "            y_pred_mode = torch.argmax(y_pred, 1)\n",
    "            label = labels[0][:-1]-1\n",
    "            correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        test_acc = correct / (len(test_loader) * 25)\n",
    "print(\"Test accuracy: {0:0.5f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "91f6d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train accuracy: 0.29077, Validation accuracy: 0.28759\n",
      "Epoch: 1, Train accuracy: 0.28538, Validation accuracy: 0.29418\n",
      "Epoch: 2, Train accuracy: 0.29302, Validation accuracy: 0.30278\n",
      "Epoch: 3, Train accuracy: 0.28482, Validation accuracy: 0.28203\n",
      "Epoch: 4, Train accuracy: 0.28620, Validation accuracy: 0.29063\n",
      "Epoch: 5, Train accuracy: 0.28657, Validation accuracy: 0.28557\n",
      "Epoch: 6, Train accuracy: 0.27775, Validation accuracy: 0.26987\n",
      "Epoch: 7, Train accuracy: 0.28670, Validation accuracy: 0.29165\n",
      "Epoch: 8, Train accuracy: 0.28720, Validation accuracy: 0.29063\n",
      "Epoch: 9, Train accuracy: 0.29058, Validation accuracy: 0.29316\n",
      "Epoch: 10, Train accuracy: 0.29014, Validation accuracy: 0.27089\n",
      "Epoch: 11, Train accuracy: 0.29271, Validation accuracy: 0.28658\n",
      "Epoch: 12, Train accuracy: 0.28833, Validation accuracy: 0.28861\n",
      "Epoch: 13, Train accuracy: 0.28720, Validation accuracy: 0.29468\n",
      "Epoch: 14, Train accuracy: 0.28482, Validation accuracy: 0.28759\n",
      "Epoch: 15, Train accuracy: 0.28951, Validation accuracy: 0.27392\n",
      "Epoch: 16, Train accuracy: 0.28833, Validation accuracy: 0.29266\n",
      "Epoch: 17, Train accuracy: 0.28563, Validation accuracy: 0.30329\n",
      "Epoch: 18, Train accuracy: 0.28626, Validation accuracy: 0.28000\n",
      "Epoch: 19, Train accuracy: 0.28582, Validation accuracy: 0.29316\n",
      "Test accuracy: 0.28691\n"
     ]
    }
   ],
   "source": [
    "autograd.set_detect_anomaly(True)\n",
    "epoch = 20\n",
    "batch_size = 8\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "test_loss = 0\n",
    "for epoch in range(epoch):\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    for (i, data) in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "        y_pred = torch.reshape(y_pred, (-1,7))\n",
    "        y_pred_mode = torch.argmax(y_pred, 1)\n",
    "        \n",
    "        y_pred = y_pred * weight[:, None]\n",
    "        label = labels[0][:-1]-1\n",
    "        correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        loss = criterion(y_pred, label)\n",
    "        loss_sum = loss_sum + loss\n",
    "        if (len(train_loader) - i) % batch_size == 1:\n",
    "            loss_sum = loss_sum / len(train_loader)\n",
    "            train_loss = loss_sum.item()\n",
    "            train_acc = correct / (len(train_loader) * 25)\n",
    "            optimizer.zero_grad()\n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for (i, data) in enumerate(val_loader):\n",
    "            count = count + 1\n",
    "            inputs, labels = data\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            y_pred = torch.reshape(y_pred, (-1,7))\n",
    "            y_pred_mode = torch.argmax(y_pred, 1)\n",
    "            label = labels[0][:-1]-1\n",
    "            correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        val_acc = correct / (len(val_loader) * 25)\n",
    "    print(\"Epoch: {0}, Train accuracy: {1:0.5f}, Validation accuracy: {2:0.5f}\".format(epoch, train_acc, val_acc))\n",
    "with torch.no_grad():\n",
    "        correct = 0\n",
    "        for (i, data) in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            y_pred = torch.reshape(y_pred, (-1,7))\n",
    "            y_pred_mode = torch.argmax(y_pred, 1)\n",
    "            label = labels[0][:-1]-1\n",
    "            correct = correct + torch.sum(label == y_pred_mode).item()\n",
    "        test_acc = correct / (len(test_loader) * 25)\n",
    "print(\"Test accuracy: {0:0.5f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ab99",
   "metadata": {},
   "source": [
    "## Train Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "06ad38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(func, x):\n",
    "    y = x[0][0]\n",
    "    y = torch.permute(y, (1,0,2))\n",
    "    y = torch.reshape(y, (-1, 8*768))\n",
    "    y = torch.stack([\n",
    "        func(x) for x in torch.unbind(y, dim=0)\n",
    "    ], dim=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "fcaabd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model,\n",
    "        input_dim,\n",
    "        inner_dim1,\n",
    "        inner_dim2,\n",
    "        num_classes,\n",
    "        activation_fn,\n",
    "        pooler_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.dropout1 = nn.Dropout(p=pooler_dropout)\n",
    "        self.dense1 = nn.Linear(input_dim, inner_dim1)\n",
    "        self.activation_fn1 = activation_fn\n",
    "        self.dropout2 = nn.Dropout(p=pooler_dropout)\n",
    "        self.dense2 = nn.Linear(inner_dim1, inner_dim2)\n",
    "        self.activation_fn2 = activation_fn\n",
    "        self.dropout3 = nn.Dropout(p=pooler_dropout)\n",
    "        self.dense3 = nn.Linear(inner_dim2, num_classes)\n",
    "        self.activation_fn3 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pretrained_model.forward(x, last_state_only=True)\n",
    "        y = reshape(self.pretrained_model.downsampling, y)\n",
    "        y = y[0]  # take <s> token (equiv. to [CLS])\n",
    "        y = self.dropout1(y)\n",
    "        y = self.dense1(y)\n",
    "        y = self.activation_fn1(y)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.dense2(y)\n",
    "        y = self.activation_fn2(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.dense3(y)\n",
    "        y = self.activation_fn3(y)\n",
    "        return y\n",
    "    \n",
    "    def freeze_pretrained_model(self):\n",
    "        for i, (name, param) in enumerate(self.pretrained_model.named_parameters()):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze_pretrained_model(self):\n",
    "        for i, (name, param) in enumerate(self.pretrained_model.named_parameters()):\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "eef75809",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "\n",
    "dataset = MyDataSet(x, y, length)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(dataset=dataset,\n",
    "                                               lengths=[train_size, val_size, test_size],\n",
    "                                               generator=generator)\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1b04de8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (pretrained_model): OctupleEncoder(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(1237, 768, padding_idx=1)\n",
       "    (embed_positions): LearnedPositionalEmbedding(8194, 768, padding_idx=1)\n",
       "    (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerSentenceEncoderLayer(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (downsampling): Sequential(\n",
       "      (0): Linear(in_features=6144, out_features=768, bias=True)\n",
       "    )\n",
       "    (upsampling): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=6144, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (dense1): Linear(in_features=768, out_features=96, bias=True)\n",
       "  (activation_fn1): Sigmoid()\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dense2): Linear(in_features=96, out_features=64, bias=True)\n",
       "  (activation_fn2): Sigmoid()\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (dense3): Linear(in_features=64, out_features=25, bias=True)\n",
       "  (activation_fn3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(pretrained_model = roberta_base1,\n",
    "                  input_dim = 768,\n",
    "                  inner_dim1 = 96,\n",
    "                  inner_dim2 = 64,\n",
    "                  num_classes = 25,\n",
    "                  activation_fn = nn.Sigmoid(),\n",
    "                  pooler_dropout = 0.2\n",
    "                  )\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer,\n",
    "                                       step_size=5, gamma=0.1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1b97f34b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.03471, Validation loss: 0.03000\n",
      "Epoch: 1, Train loss: 0.03036, Validation loss: 0.02775\n",
      "Epoch: 2, Train loss: 0.02901, Validation loss: 0.02708\n",
      "Epoch: 3, Train loss: 0.02858, Validation loss: 0.02687\n",
      "Epoch: 4, Train loss: 0.02833, Validation loss: 0.02681\n",
      "Epoch: 5, Train loss: 0.02844, Validation loss: 0.02680\n",
      "Epoch: 6, Train loss: 0.02837, Validation loss: 0.02680\n",
      "Epoch: 7, Train loss: 0.02837, Validation loss: 0.02680\n",
      "Epoch: 8, Train loss: 0.02852, Validation loss: 0.02680\n",
      "Epoch: 9, Train loss: 0.02831, Validation loss: 0.02680\n",
      "Epoch: 10, Train loss: 0.02819, Validation loss: 0.02680\n",
      "Epoch: 11, Train loss: 0.02825, Validation loss: 0.02680\n",
      "Epoch: 12, Train loss: 0.02838, Validation loss: 0.02680\n",
      "Epoch: 13, Train loss: 0.02830, Validation loss: 0.02680\n",
      "Epoch: 14, Train loss: 0.02851, Validation loss: 0.02680\n",
      "Epoch: 15, Train loss: 0.02841, Validation loss: 0.02680\n",
      "Epoch: 16, Train loss: 0.02829, Validation loss: 0.02680\n",
      "Epoch: 17, Train loss: 0.02835, Validation loss: 0.02680\n",
      "Epoch: 18, Train loss: 0.02843, Validation loss: 0.02680\n",
      "Epoch: 19, Train loss: 0.02820, Validation loss: 0.02680\n",
      "Test loss: 0.02533\n"
     ]
    }
   ],
   "source": [
    "model.freeze_pretrained_model()\n",
    "autograd.set_detect_anomaly(True)\n",
    "epoch = 20\n",
    "batch_size = 4\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "test_loss = 0\n",
    "count = 0\n",
    "for epoch in range(epoch):\n",
    "    loss_sum = 0\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for (i, data) in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        label = labels[0][:-1]\n",
    "        y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "        loss_sum = loss_sum + criterion(y_pred, label)\n",
    "        count = count + 1\n",
    "        if (len(train_loader) - i) % batch_size == 1:\n",
    "            train_loss = train_loss + loss_sum.item()\n",
    "            loss = loss_sum / count\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum = 0\n",
    "            count = 0\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        for (i, data) in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            label = labels[0][:-1]\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            loss = criterion(y_pred, label)\n",
    "            loss_sum = loss_sum + loss.item()\n",
    "        val_loss = loss_sum / len(val_loader)\n",
    "    print(\"Epoch: {0}, Train loss: {1:0.5f}, Validation loss: {2:0.5f}\".format(epoch, train_loss, val_loss))\n",
    "    scheduler.step()\n",
    "with torch.no_grad():\n",
    "        loss_sum = 0\n",
    "        for (i, data) in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            label = labels[0][:-1]\n",
    "            y_pred = torch.Tensor(model(inputs.squeeze(dim = 0)))\n",
    "            loss = criterion(y_pred, label)\n",
    "            loss_sum = loss_sum + loss.item()\n",
    "        test_loss = loss_sum / len(test_loader)\n",
    "print(\"Test loss: {0:0.5f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98c6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
